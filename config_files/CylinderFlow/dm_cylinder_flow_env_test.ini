[DATA]
# dataset class
class_name = DM_CylinderFlowDataset
name = DeepMind_CylinderFlow

# graph construction parameters
# node types:
# 0 normal
# 4 source
# 5 outflow
# 6 wall
normal_node_types = [0]
boundary_node_types = [5, 6]
source_node_types = [4]
num_node_types = 4
graph_type = radius
k = 10
radius = 0.01

# processing params
output_type = velocity
window_length = 1
apply_onehot = True
apply_partitioning = True
noise = [0.02, 0.02]
noise_gamma = 0.1
normalize = True

[MODEL]
mgn_dim = 128
mp_iterations = 15
mlp_norm_type = LayerNorm
normalize_output = True

# can be one of [FullResidualConnection, ResidualConnection, InitialConnection, DenseConnection]
connection_type = FullResidualConnection
# for ResidualConnection & InitialConnection types only, can be float in range [0, 1]
connection_alpha = 0.5
# for DenseConnection type only, can be one of [concat, maxpool, attention]
connection_aggregation = concat

[TRAINING]
# hyperparams
epochs = 100
batch_size = 2
grad_accum_steps = 1
lr = 5e-3
scheduler = ExpLR
use_parallel = False
use_amp = False

# selective backprop flag. "dataset_idx" must be a graph attribute or "Data" field in the returned sample in a Dataset's __getitem__
use_sbp = False
# start sbp after n full epochs
sbp_start_epoch = 1
# every (n-1) epochs will be a sbp epoch, afterwards (epoch % n == 0) will be a full epoch
sbp_rate = 3
# percent of high-loss samples to select for sbp epochs
sbp_percent = 0.25
# of the sbp_percent, randomly remove/add samples. helps avoid re-using bad samples that consistently have high loss.
sbp_randomness = 0.05

# can be one of [last_epoch, best_val]
load_prefix = last_epoch

# logging
expt_name = dm_cylinder_flow_exp
log_rate = 100
use_tensorboard = True
tb_rate = 10
# expt_name = overfitddp_momentum${DATA:momentum}_bz${TRAINING:batch_size}_${DATA:noise}_noisegamma_${DATA:noise_gamma}_name_${DATA:name}_mgndim_${MODEL:mgn_dim}_mpiterations_${MODEL:mp_iterations}_${TRAINING:scheduler}_${TRAINING:lr}_${DATA:sim_range}_DP_${TRAINING:use_parallel}_DS_${DATA:class_name}_ddphook_${TRAINING:ddp_hook}
train_output_dir = /p/vast1/saini5/CCSI/output_exp_dir/train
checkpoint_dir = ${TRAINING:train_output_dir}/checkpoints/${TRAINING:expt_name}
log_dir = ${TRAINING:train_output_dir}/logs/${TRAINING:expt_name}
tensorboard_dir = ${TRAINING:train_output_dir}/tensorboard/${TRAINING:expt_name}

# ddp
# 'mean' for default DDP behavior, or 'sum'
ddp_hook = mean
pin_memory = True

# world_size should be the total number of gpus across all nodes (1 process per gpu)
world_size = 1
data_num_workers = 0

[SCHEDULER]
# ExpLR
scheduler_decay_interval = 0.5
gamma = 0.01
min_lr = 1e-8
# uncomment below to manually set decay steps, otherwise it'll automatically compute it.
# decay_steps = 4e6

# OneCycle
max_lr = 3e-2

# ReduceLROPlateau
patience = 50

[PARTITIONING]
partitioning_method = "grid"
padding = ${MODEL:mp_iterations}
# grid
# partition graph into grid (nrows x ncols) cells
nrows = 2
ncols = 2
# range
# specify the boundaries of rectangular patches in d dimensions
# each range_i is a list of 2-tuples definining the patches
# all range_i must have the same length
# graph will be partitioned into N patches, where N is the size of
# range_i. The kth patch consists of the nodes in
# the rectangle with corners 
# (range_0[k][0], range_1[k][0], ..., range_d[k][0]), ...
# (range_0[k][1], range_1[k][1], ..., range_d[k][1])
#range_0 = [(0, 0.8), (0.8, 1.6), (0, 0.8), (0.8, 1.6)]
#range_1 = [(0, 0.205), (0, 0.205), (0.205, 0.41), (0.205, 0.41)]

[TESTING]
do_rollout_test = True
rollout_start_idx = 100
test_output_dir = /p/vast1/saini5/CCSI/output_exp_dir/test
outfile = ${TESTING:test_output_dir}/rollout.pk
batch_size = ${TRAINING:batch_size}

# --------------- EXTRA PARAMS (auto forwarded via **kwargs) ---------------
[DM_CYLINDER_FLOW_PARAMS]
train_input_dir = '/usr/workspace/ccsi/ml4cfd/data/cylinder_flow/train/'
train_sample_idx_start = 0
train_sample_idx_end = 30
test_input_dir = '/usr/workspace/ccsi/ml4cfd/data/cylinder_flow/valid/'
test_sample_idx_start = 0
test_sample_idx_end = 5
node_types_relabel = [0, 4, 5, 6]
force_preprocess = False
train_processed_dir = /p/vast1/saini5/cylinder_flow/preprocessed/train/
test_processed_dir = /p/vast1/saini5/cylinder_flow/preprocessed/valid/
